apiVersion: batch/v1
kind: Job
metadata:
    name: llama2-7b-lora-pytorch-job
    labels:
        kueue.x-k8s.io/queue-name:  eidf114ns-user-queue
spec:
    completions: 1
    template:
        metadata:
            name: llama2-7b-lora-pytorch-pod
        spec:
            restartPolicy: Never
            containers:
            - name: llama2-7b-lora-pytorch-con
              image: charaka/llama2_pytorch:v1.0-llama2-lora
              command: ["python3"]
              args: ["/mnt/ceph_rbd/Llama_2_7b_lora_v4.py"]
              volumeMounts:
                - mountPath: /mnt/ceph_rbd
                  name: volume
              resources:
                requests:
                  cpu: 2
                  memory: "64Gi"
                limits:
                  cpu: 4
                  memory: "64Gi"
                  nvidia.com/gpu: 1
            nodeSelector:
                nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
            volumes:
                - name: volume
                  persistentVolumeClaim:
                    claimName: llama2-7b-pytorch-pvc

